---
title: "HR Efficiency Analysis"
date: "`r format(Sys.Date())`"
output: 
  html_document:
    keep_md: true
    fig_width: 14
    fig_height: 8
    out_width: '900px'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)
library(caret)
library(dplyr)
library(ggplot2)
library(gridExtra)
library(RColorBrewer)
library(arm)
library(cluster)
myTheme <- theme( panel.grid = element_blank(), legend.key = element_blank(), plot.title = element_text(hjust = 0.5))
```

## Problem
###Factors important to retain performing employees
* Why are our best and most experienced employees leaving prematurely? 
We will analysing the data available in hand to identify avenues to improving hr efficiency.
* Can we predict which valuable employees will leave next?
We will be building a predictive model to determine how long employee would stay and their probability in leaving.

## Data Source
https://www.kaggle.com/ludobenistant/hr-analytics

https://www.kaggle.com/ludobenistant/hr-analytics/downloads/human-resources-analytics.zip

Lets load the dataset
```{r Loading Data }
  hr <- read.csv("./HR_comma_sep.csv", header = TRUE, stringsAsFactors = FALSE)
```

#####Lets look at the fields in the dataset

* _satisfaction_level_ - Level of satisfaction (0-1)
* _last_evaluation_ - Evaluation of employee performance (0-1)
* _number_project_ - Number of projects completed while at work
* _average_montly_hours_ - Average monthly hours at workplace
* _time_spend_company_ - Number of years spent in the company
* _Work_accident_ - Whether the employee had a workplace accident
* _left_ - Whether the employee left the workplace or not (1 or 0) 
* _promotion_last_5years_ - Whether the employee was promoted in the last five years
* _sales_ - Department in which they work for
* _salary_ - Salary (High, medium, Low)

Lets analyze the structure of dataset
```{r data_str}
str(hr)
```

####Observations
* Fields _number_project_, _promotion_last_5years_, _left_, _Work_accident_, _sales_ have discrete values.

```{r salary_str}
str(as.factor(hr$salary))
```

####Observations
* Field _salary_ is also discrete with "high", "medium" & "low". Also looks like "low" is assigned 2, "medium" is assigned 3 and "high" is assigned 1.

Lets look at all unique values for field 'sales'.
```{r sales_str}
unique(hr$sales)
```

####Observations
* Field _sales_ does not seems to have sales figures but departments that employee belongs to.


Lets look at the summary of the data set to see if there are no invalid data
```{r }
  summary(hr)
```
####Observations
* All fields in the dataset have non-NA values. 


## Data Wrangling

Rename the _sales_ field to _dept_ and create seperate field salary_level 3: High, 2: Medium & 1: low
```{r}
names(hr)[9] <- "dept"
hr <- hr %>% mutate(salary_level = case_when(
  .$salary == "high" ~ 3, 
  .$salary == "medium" ~ 2, 
  .$salary == "low" ~ 1))
```

Lets look are structure once again
```{r}
  str(hr)
```

## Data Exploration

Lets analyze _satisfaction_level_, _time_spend_company_, _last_evaluation_, _average_monthly_hours_, _work_accident_, _salary_ and _number_project_


```{r plot_data_exp_analysis}
hr_left <- hr %>% filter(left == 1)

satis_l <- hr_left %>% ggplot(aes(satisfaction_level)) +
  geom_histogram( binwidth = 0.05, aes(fill = ..count..)) +
  scale_fill_gradient("Count", low = "green", high = "red") +
  labs(x = "satisfaction_level", y = "employees", title = "satisfaction level") + myTheme

tm_spnd <- hr_left %>% ggplot(aes(time_spend_company)) +
  geom_histogram( binwidth = 0.05, aes(fill = ..count..)) +
  scale_fill_gradient("Count", low = "green", high = "red") +
  labs(x = "time_spend_company", y = "employees", title = "Time Spend in Company") + myTheme

lst_eval <- hr_left %>% ggplot(aes(last_evaluation)) +
  geom_histogram( binwidth = 0.05, aes(fill = ..count..)) +
  scale_fill_gradient("Count", low = "green", high = "red") +
  labs(x = "last_evaluation", y = "employees", title = "Last evaluation") + myTheme

mnthly_hrs <- hr_left %>% ggplot(aes(average_montly_hours)) +
  geom_histogram( binwidth = 0.05, aes(fill = ..count..)) +
  scale_fill_gradient("Count", low = "green", high = "red") +
  labs(x = "average_montly_hours", y = "employees", title = "Average montly hours") + myTheme

wrk_accdnt <- hr_left %>% ggplot(aes(Work_accident)) +
  geom_histogram( binwidth = 0.05, aes(fill = ..count..)) +
  scale_fill_gradient("Count", low = "green", high = "red") +
  labs(x = "Work_accident", y = "employees", title = "Work accident") + myTheme

sal <- hr_left %>% ggplot(aes(salary_level)) +
  geom_histogram( binwidth = 0.05, aes(fill = ..count..)) +
  scale_fill_gradient("Count", low = "green", high = "red") +
  labs(x = "salary", y = "employees", title = "Salary") + myTheme

nmbr_prj <- hr_left %>% ggplot(aes(as.numeric(number_project))) +
  geom_histogram( binwidth = 0.05, aes(fill = ..count..)) +
  scale_fill_gradient("Count", low = "green", high = "red") +
  labs(x = "number_project", y = "employees", title = "Number of projects") + myTheme

grid.arrange(satis_l, tm_spnd, lst_eval,mnthly_hrs,wrk_accdnt,sal,nmbr_prj, nrow = 4)
```

####Notice high number of employees leaving the company 
* had been with company for less than 3 years
* had an evaluation rating less than 0.5
* had an average monthly hours in work at less than 170 hrs
* had 2 projects or less
* Overall lower performing employees are leaving more. This warrants improvement in hiring process to avoid low performers
* Aside from low performers, we can notice number of employees leaving creeeping up among mid to high performing. This is an area that needs to be also looked into for reduction in rate of attrition.  



##Regression Model

###Linear Regression
Lets build a model to determine how long an employee will stay

####Train Data
```{r}

set.seed(3456)
trainIndex <- createDataPartition(hr$time_spend_company, p = .8, 
                                  list = FALSE, 
                                  times = 1)
head(trainIndex)

hrTrain <- hr[ trainIndex,]

```
###Test Data
```{r}
hrTest  <- hr[-trainIndex,]
```

###Models
####Lets use caret package to anlyze significant fields for linear model
```{r fields_lm}
plot(varImp(train(time_spend_company ~ ., data = hrTrain, method = "lm")))
```

```{r}
lm_time_spend <- lm(time_spend_company~left+ dept+ promotion_last_5years + number_project+last_evaluation+salary+Work_accident+satisfaction_level, data = hrTrain)
lm_time_spend_summary <- summary(lm_time_spend)
lm_time_spend_summary
```

```{r}
lm_time_spend_mthly_hrs <- lm(time_spend_company~left+ dept+ promotion_last_5years + number_project+last_evaluation+average_montly_hours+salary+Work_accident+satisfaction_level, data = hrTrain)
lm_time_spend_mthly_hrs_summary <- summary(lm_time_spend_mthly_hrs)
lm_time_spend_mthly_hrs_summary
```

###Lets compare the two models
```{r}
lm_model_anova <- anova(lm_time_spend,lm_time_spend_mthly_hrs)
lm_model_anova
```

_lm_time_spend_ has an R Squared of `r lm_time_spend_summary$r.squared` and adjusted R-Squared of `r lm_time_spend_summary$adj.r.squared`

_lm_time_spend_mthly_hrs_ has an R Squared of `r lm_time_spend_mthly_hrs_summary$r.squared` and adjusted R-Squared of `r lm_time_spend_mthly_hrs_summary$adj.r.squared`



#####Based on anova and both R Squared and Adjusted R Squared model _lm_time_spend_mthly_hrs_ seems to be a better fit


###Predict using the model _lm_time_spend_mthly_hrs_
```{r}
predict_lm_emp_leaving <- predict(lm_time_spend_mthly_hrs, newdata = hrTest)
#summary of prediction
summary(predict_lm_emp_leaving)

#summary of actuals
summary(hrTest$time_spend_company)
```

```{r predict_vs_actual_df}
#Create data frame actual vs predicted
p <- data.frame(predict_lm_emp_leaving)
p$ID <- as.numeric(rownames(p))
p$Type <- 'Predicted'
names(p)[1] <- 'time_spend_company'

a <- data.frame(hrTest$time_spend_company)
a$ID <- as.numeric(rownames(hrTest))
a$Type <- 'Actual'
names(a)[1] <- 'time_spend_company'

m <- data.frame(rbind(a,p))
```

```{r plot_predict_vs_actual_df}
#plot predict and actual
m %>% ggplot(aes(x = ID, y = time_spend_company, col = Type)) + 
  geom_line() + 
  geom_smooth() +
  labs(x = "Observation", y = "Number of years spent in the company", title = "Plot Predicted & Actual Time spend vs Observation") +
  scale_colour_brewer(palette = brewer.pal(11,"Spectral")) +
  myTheme
```

```{r compute_correlation_coef}
#correlation data
plot_data <- data.frame(predict_lm_emp_leaving, hrTest$time_spend_company)
names(plot_data)[1] <- "predicted"
names(plot_data)[2] <- "actual"

correlation <- cor(plot_data)
correlation

#correlation
correlation[1,2]
```

```{r compute_rmse}
#lets see how good the model prediction was

#Sum of squared errors(SSE)
sse = sum((hrTest$time_spend_company - predict_lm_emp_leaving ) ^ 2)
round(sse, digits = 2)

#Root mean squared errors (RMSE)
rmse = sqrt(sse / nrow(hrTest))
round(rmse, digits = 2)

```
####_lm_time_spend_mthly_hrs_ prediction has an 'Root mean squared errors' `r round(rmse, digits = 2)` and correlation coefficient of `r round(correlation[1,2], digits =2)`

###Logistic Regression
Lets build a model to predict if the employee will leave

####Train Data
```{r}

set.seed(3456)
trainIndex <- createDataPartition(hr$left, p = .8, 
                                  list = FALSE, 
                                  times = 1)
head(trainIndex)

hrTrain <- hr[ trainIndex,]

```
###Test Data
```{r}
hrTest  <- hr[-trainIndex,]
```

###Models

#####Significant fields

```{r log_reg_model_signi_fields}
plot(varImp(train(left ~ ., data = hrTrain, method = "bayesglm")))
```

#####Significant fields
* _satisfaction_level_, _Work_accident_, _salary_, _time_spend_company_, _number_project_, _average_montly_hours_, _promotion_last_5years_, _dept_

```{r log_model}
fitControl <- trainControl(method = "bayesglm",
                           number = 10,repeats = 10)
log_model <- train(left ~ satisfaction_level + Work_accident  + salary + time_spend_company + number_project + average_montly_hours + promotion_last_5years + dept, data = hrTrain, method = "bayesglm" )

summary(log_model)
```

```{r}
#Lets test prediction using test data 
testPredication <- predict(log_model,hrTest)
#d <- table(hrTest$left, testPredication)

defaultSummary(data.frame(obs = hrTest$left, pred = testPredication))
```

###Cluster Analysis
```{r cluster}
wssplot <- function(data, nc = 15, seed = 1234) {
  wss <- (nrow(data) - 1) * sum(apply(data, 2, var))
  for (i in 2:nc) {
    set.seed(seed)
    wss[i] <- sum(kmeans(data, centers = i)$withinss)
  }
  plot(1:nc,
       wss,
       type = "b",
       xlab = "Number of Clusters",
       ylab = "Within groups sum of squares")
  print(wss[1])
}

#lets make all fields numeric
hr_clust <- hr
hr_clust$dept <- as.numeric( as.factor(hr_clust$dept))
hr_clust$salary <- as.numeric( as.factor(hr_clust$salary))
hr_clust$number_project <- as.numeric( hr_clust$number_project)
hr_clust$Work_accident <- as.numeric( hr_clust$Work_accident)
hr_clust$left <- as.numeric(hr_clust$left)
hr_clust$promotion_last_5years <- as.numeric(hr_clust$promotion_last_5years)

#lets identify optimum number of clusters
wssplot(scale(hr_clust))

# there are two turns one at 4 and another at 7

fit.km <- kmeans(scale(hr_clust), 3)
clusplot(hr_clust,fit.km$cluster)

fit.km <- kmeans(scale(hr_clust), 4)
clusplot(hr_clust,fit.km$cluster)

fit.km <- kmeans(scale(hr_clust), 7)
clusplot(hr_clust,fit.km$cluster)

fit.km <- kmeans(scale(hr_clust), 10)
clusplot(hr_clust,fit.km$cluster)
```

