---
title: "HR Efficiency Analysis"
date: "`r format(Sys.Date())`"
output: 
  html_document:
    keep_md: true
    fig_width: 14
    fig_height: 8
    out_width: '900px'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)
library(caret)
library(dplyr)
library(ggplot2)
library(gridExtra)
library(RColorBrewer)
library(arm)
library(cluster)
library(GGally)
myTheme <- theme( panel.grid = element_blank(), legend.key = element_blank(), plot.title = element_text(hjust = 0.5))
```

## Problem
###Factors important to retain performing employees
* We will be analysing the data available in hand to identify avenues to improving hr efficiency.
* We will be building a predictive model to determine how long employee would stay and their probability in leaving.

## Data Source
https://www.kaggle.com/ludobenistant/hr-analytics

https://www.kaggle.com/ludobenistant/hr-analytics/downloads/human-resources-analytics.zip

#####Lets load the dataset
```{r Loading Data }
hr <- read.csv("./HR_comma_sep.csv", header = TRUE, stringsAsFactors = FALSE)
#hr_corrplot = hr
summary(hr)
```

* _satisfaction_level_ - Employee Level of satisfaction. It ranges from __0 (low satisfaction)__ to __1 (high satisfaction)__
* _last_evaluation_ - Latest performance evaluation rating of employee. It ranges from __0 (low rating)__ to __1 (high rating)__
* _number_project_ - Number of projects completed while at work. It varies from __2__ to __7__ projects, on an average an employee has worked on __4__ projects.
* _average_montly_hours_ - Average monthly work hours at workplace. It varies from __96 hrs__ to __310 hrs__ with a mean of __201 hrs__ per month.
* _time_spend_company_ - Number of years spent in the company. It ranges from __2__ years to __10__ years, with an average of around __4__ years until now or until they left.
* _Work_accident_ - Whether the employee had a workplace accident
* _left_ - Whether the employee left the workplace or not. __1__ means __left__ and __0__ means still with company. Probability of leaving is __0.23__.
* _promotion_last_5years_ - Whether the employee was promoted in the last five years. __1__ means promoted and __0__ means no promotion in last 5 years. Probability of promotion in last 5 years is __0.02__.


* _sales_ - Department in which they work for. _accounting_, _hr_, _IT_, _management_,  _marketing_, _product_mng_, _RandD_, _sales_, _support_, _technical_ are various departments employees belong to.
```{r summary_sales }
summary(as.factor(hr$sales))
```

* _salary_ - Salary as high, medium & low.
```{r summary_salary}
summary(as.factor(hr$salary))
```

* Fields _number_project_, _promotion_last_5years_, _left_, _Work_accident_, _sales_,_salary_ have discrete values.
* Looking at the summary of the data set there are no bad data , 'blanks', 'NA', null etc.

## Data Wrangling

Rename the _sales_ column to _dept_ and make columns _number_project_, _promotion_last_5years_, _left_, _Work_accident_, _sales_,_salary_ as factor
```{r}

hr$number_project <- as.factor(hr$number_project)
hr$promotion_last_5years <- as.factor(hr$promotion_last_5years)
hr$left <- as.factor(hr$left)
hr$Work_accident <- as.factor(hr$Work_accident)
hr$sales <- as.factor(hr$sales)
hr$salary <- as.factor(hr$salary)

names(hr)[9] <- "dept"

```

Lets look are structure
```{r}
  str(hr)
```

## Data Exploration

Lets look at correaltion between these various columns

```{r corr_plot}
hr %>%  dplyr::select(
  satisfaction_level,
  last_evaluation,
  average_montly_hours,
  number_project,
  left
  ) %>% ggpairs()

hr %>%  dplyr::select(Work_accident, promotion_last_5years, salary, left) %>% ggpairs()

```

* There is strong correlation between _last_evaluation_, _number_project_, _average_monthly_hours_.
* There is also strong correlation between _satisfaction_level_ & _left_.



Lets analyze _satisfaction_level_, _time_spend_company_, _last_evaluation_, _average_monthly_hours_, _work_accident_, _salary_ and _number_project_


```{r plot_data_exp_analysis}

satis_l <- hr %>% ggplot(aes(satisfaction_level)) +
  geom_histogram( binwidth = 0.05, aes(fill = left)) +
  labs(x = "satisfaction_level", y = "employees", title = "satisfaction level") + myTheme

lst_eval <- hr %>% ggplot(aes(last_evaluation)) +
  geom_histogram( binwidth = 0.05, aes(fill = left)) +
  labs(x = "last_evaluation", y = "employees", title = "Last evaluation") + myTheme

tm_spnd <- hr %>% ggplot(aes(time_spend_company)) +
  geom_histogram( binwidth = 1, aes(fill = left)) +
  labs(x = "time_spend_company", y = "employees", title = "Time Spend in Company") + myTheme


mnthly_hrs <- hr %>% ggplot(aes(average_montly_hours)) +
  geom_histogram( binwidth = 1, aes(fill = left)) +
  labs(x = "average_montly_hours", y = "employees", title = "Average montly hours") + myTheme

wrk_accdnt <- hr %>% ggplot(aes(Work_accident)) +
  geom_histogram( binwidth = 0.05, aes(fill = left), stat = "count") +
  labs(x = "Work_accident", y = "employees", title = "Work accident") + myTheme

sal <- hr %>% ggplot(aes(salary)) +
  geom_histogram( binwidth = 0.05, aes(fill = left), stat = "count") +
  labs(x = "salary", y = "employees", title = "Salary") + myTheme

nmbr_prj <- hr %>% ggplot(aes(as.numeric(number_project))) +
  geom_histogram( binwidth = 1, aes(fill = left)) +
  labs(x = "number_project", y = "employees", title = "Number of projects") + myTheme

grid.arrange(satis_l, lst_eval, sal, tm_spnd, mnthly_hrs,nmbr_prj, wrk_accdnt, nrow = 4)
```

* Using plots for __avaerage_monthly_hours__, __last_evaluation__ & __number_projects__,
  + Overall lower performing employees are leaving more. This warrants improvement in hiring process to avoid low performers
  + Aside from low performers, we can notice number of employees leaving creeeping up among mid to high performing. This is an area that needs to be also looked into for reduction in rate of attrition.  

* Most employees seem to have had 2- 4 projects. It looks like employees that remain having more than 4 are very low.
* Average monthly hours of 160 to 215 hrs or less, seems to show very low attrition.
* Most employees have been with company 5 or less.


###Cluster Analysis
```{r cluster1}
#lets identify optimum number of clusters
wssplot <- function(data, nc = 15, seed = 1234) {
  wss <- (nrow(data) - 1) * sum(apply(data, 2, var))
  for (i in 2:nc) {
    set.seed(seed)
    wss[i] <- sum(kmeans(data, centers = i)$withinss)
  }
  plot(1:nc,
       wss,
       type = "b",
       xlab = "Number of Clusters",
       ylab = "Within groups sum of squares")   
}

#lets make all fields numeric
hr_clust <- hr
hr_clust$dept <- as.numeric( as.factor(hr_clust$dept))
hr_clust$salary <- as.numeric( as.factor(hr_clust$salary))
hr_clust$number_project <- as.numeric( hr_clust$number_project)
hr_clust$Work_accident <- as.numeric( hr_clust$Work_accident)
hr_clust$left <- as.numeric(hr_clust$left)
hr_clust$promotion_last_5years <- as.numeric(hr_clust$promotion_last_5years)

wssplot(scale(hr_clust))
```

```{r clustering}
# lets consider 9 clusters 

fit.km <- kmeans(scale(hr_clust), 9)
clusplot(hr_clust,fit.km$cluster)
```


##Regression Model

###Linear Regression
Lets build a model to determine how long an employee will stay


####Train Data
```{r}

set.seed(3456)
trainIndex <- createDataPartition(hr$time_spend_company, p = .8, 
                                  list = FALSE, 
                                  times = 1)
head(trainIndex)

hrTrain <- hr[ trainIndex,]

```
###Test Data
```{r}
hrTest  <- hr[-trainIndex,]
```

###Models
####Lets use caret package to anlyze significant fields for linear model
```{r fields_lm}
plot(varImp(train(time_spend_company ~ ., data = hrTrain, method = "lm")))
```

```{r}
lm_time_spend <- lm(time_spend_company~left+ dept+ promotion_last_5years + number_project+last_evaluation+salary+Work_accident+satisfaction_level, data = hrTrain)
lm_time_spend_summary <- summary(lm_time_spend)
lm_time_spend_summary
```

```{r}
lm_time_spend_mthly_hrs <- lm(time_spend_company~left+ dept+ promotion_last_5years + number_project+last_evaluation+average_montly_hours+salary+Work_accident+satisfaction_level, data = hrTrain)
lm_time_spend_mthly_hrs_summary <- summary(lm_time_spend_mthly_hrs)
lm_time_spend_mthly_hrs_summary
```

###Lets compare the two models
```{r}
lm_model_anova <- anova(lm_time_spend,lm_time_spend_mthly_hrs)
lm_model_anova
```

_lm_time_spend_ has an R Squared of `r lm_time_spend_summary$r.squared` and adjusted R-Squared of `r lm_time_spend_summary$adj.r.squared`

_lm_time_spend_mthly_hrs_ has an R Squared of `r lm_time_spend_mthly_hrs_summary$r.squared` and adjusted R-Squared of `r lm_time_spend_mthly_hrs_summary$adj.r.squared`


#####Based on anova and both R Squared and Adjusted R Squared model _lm_time_spend_mthly_hrs_ seems to be a better fit. However R-Squared is very low and close to 0, model is a poor fit.


###Predict using the model _lm_time_spend_mthly_hrs_
```{r}
predict_lm_emp_leaving <- predict(lm_time_spend_mthly_hrs, newdata = hrTest)
#summary of prediction
summary(predict_lm_emp_leaving)

#summary of actuals
summary(hrTest$time_spend_company)
```

```{r predict_vs_actual_df}
#Create data frame actual vs predicted
p <- data.frame(predict_lm_emp_leaving)
p$ID <- as.numeric(rownames(p))
p$Type <- 'Predicted'
names(p)[1] <- 'time_spend_company'

a <- data.frame(hrTest$time_spend_company)
a$ID <- as.numeric(rownames(hrTest))
a$Type <- 'Actual'
names(a)[1] <- 'time_spend_company'

m <- data.frame(rbind(a,p))
```

```{r plot_predict_vs_actual_df}
#plot predict and actual
m %>% ggplot(aes(x = ID, y = time_spend_company, col = Type)) + 
  geom_line() + 
  geom_smooth() +
  labs(x = "Observation", y = "Number of years spent in the company", title = "Plot Predicted & Actual Time spend vs Observation") +
  scale_colour_brewer(palette = brewer.pal(11,"Spectral")) +
  myTheme
```

```{r compute_correlation_coef}
#correlation data
plot_data <- data.frame(predict_lm_emp_leaving, hrTest$time_spend_company)
names(plot_data)[1] <- "predicted"
names(plot_data)[2] <- "actual"

plot_data$predicted <- round(plot_data$predicted)

correlation <- cor(plot_data)
correlation

#correlation
correlation[1,2]
```

The correlation coefficient is low __`r correlation[1,2]`__

```{r compute_rmse}
#lets see how good the model prediction was

#Sum of squared errors(SSE)
sse = sum((hrTest$time_spend_company - predict_lm_emp_leaving ) ^ 2)
round(sse, digits = 2)

#Root mean squared errors (RMSE)
rmse = sqrt(sse / nrow(hrTest))
round(rmse, digits = 2)

```
####_lm_time_spend_mthly_hrs_ prediction has an 'Root mean squared errors' `r round(rmse, digits = 2)` and correlation coefficient of `r round(correlation[1,2], digits =2)`

###Logistic Regression
Lets build a model to predict if the employee will leave

####Train Data
```{r}

set.seed(3456)
trainIndex <- createDataPartition(hr$left, p = .8, 
                                  list = FALSE, 
                                  times = 1)
head(trainIndex)

hrTrain <- hr[ trainIndex,]

```
###Test Data
```{r}
hrTest  <- hr[-trainIndex,]
```

###Models

#####Significant fields

```{r log_reg_model_signi_fields}
plot(varImp(train(left ~ ., data = hrTrain, method = "bayesglm")))
```

#####Significant fields
* _satisfaction_level_, _Work_accident_, _salary_, _time_spend_company_, _number_project_, _average_montly_hours_, _promotion_last_5years_, _dept_

```{r log_model}
fitControl <- trainControl(method = "bayesglm",
                           number = 10,repeats = 10)
log_model <- train(left ~ satisfaction_level + Work_accident  + salary + time_spend_company + number_project + average_montly_hours + promotion_last_5years + dept, data = hrTrain, method = "bayesglm" )
glm_prediction_train <- predict(log_model, newdata = hrTrain)
glm_confustion_matrix_train <- confusionMatrix(glm_prediction_train, hrTrain$left, dnn = c("Predicted","actual"))
summary(log_model)
glm_confustion_matrix_train
```


```{r}
#Lets test prediction using test data 
testPredication <- predict(log_model,hrTest)
glm_confustion_matrix_test <- confusionMatrix(testPredication,hrTest$left, dnn = c("Predicted","actual"))

glm_confustion_matrix_test
```

```{r rf}
library(doMC)
registerDoMC(5)
hrTrain_1 <- hrTrain[createDataPartition(y = hrTrain$left,p = 0.3,list = FALSE),]
rf_model <- train(left~.,data = hrTrain_1,method = "rf",
                trControl = trainControl(method = "cv",number = 5),
                pallowParallel = TRUE)
print(rf_model)
rf_prediction_train = predict(rf_model, newdata = hrTrain)

rf_conf_matrix_train <- confusionMatrix(rf_prediction_train, hrTrain$left)
rf_conf_matrix_train

rf_prediction = predict(rf_model, newdata = hrTest)

rf_conf_matrix <- confusionMatrix(rf_prediction, hrTest$left)
rf_conf_matrix

```

#####Random Forest analysis has provided a very good model. 
* __Sensitivity__ : `r rf_conf_matrix$byClass["Sensitivity"]`
* __Specificity__ : `r rf_conf_matrix$byClass["Specificity"]`
* __Accuracy__    : `r rf_conf_matrix$overall["Accuracy"]`
* __Kappa__       : `r rf_conf_matrix$overall["Kappa"]`

```{r reg_summary}
overall <- cbind(
  glm_confustion_matrix_train$overall,
  glm_confustion_matrix_test$overall,
  rf_conf_matrix_train$overall,
  rf_conf_matrix$overall
  )
  
byClass <- cbind(
  glm_confustion_matrix_train$byClass,
  glm_confustion_matrix_test$byClass,
  rf_conf_matrix_train$byClass,
  rf_conf_matrix$byClass
  )
  
all_conf_matrix <- rbind(overall, byClass)
all_conf_matrix <- data.frame(all_conf_matrix)
names(all_conf_matrix) <- c("glm_train", "glm_test", "rf_train", "rf_test")

all_conf_matrix$glm_train <- round(all_conf_matrix$glm_train, digits = 4)
all_conf_matrix$glm_test <- round(all_conf_matrix$glm_test, digits = 4)
all_conf_matrix$rf_train <- round(all_conf_matrix$rf_train, digits = 4)
all_conf_matrix$rf_test <- round(all_conf_matrix$rf_test, digits = 4)

all_conf_matrix

```




